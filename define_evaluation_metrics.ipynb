{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# from langchain.evaluation.criteria import {\n",
    "#     CriteriaEvalChain,\n",
    "#     LabeledCriteriaEvalChain\n",
    "# }\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", max_tokens=4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultScore(BaseModel):\n",
    "    score: float = Field(..., description=\"The score of the result, ranging from 0 to 1 where 1 is the best possible score.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_prompt = PromptTemplate(\n",
    "input_variables=[\"question\", \"ground_truth\", \"generated_answer\"],\n",
    "template=\"\"\"\n",
    "Question: {question}\n",
    "Ground Truth: {ground_truth}\n",
    "Generated Answer: {generated_answer}\n",
    "\n",
    "Evaluate the correctness of the generated answer compared to the ground truth.\n",
    "Score from 0 to 1, where 1 is perfectly correct and 0 is completely incorrect.\n",
    "any score between 0 and 1 is acceptable and depends on how correct the generated answer is.\n",
    "\n",
    "Score:\n",
    "\"\"\"\n",
    ")\n",
    "correctness_chain = correctness_prompt | llm.with_structured_output(ResultScore)\n",
    "\n",
    "\n",
    "def evaluate_correctness(question, ground_truth, generated_answer):\n",
    "    \"\"\"Evaluates the correctness of the generated answer compared to the ground truth.\n",
    "\n",
    "    Args:\n",
    "        question: The question.\n",
    "        ground_truth: The ground truth answer.\n",
    "        generated_answer: The generated answer.\n",
    "\n",
    "    Returns:\n",
    "        A float between 0 and 1, where 1 is the best possible score.\n",
    "    \"\"\"\n",
    "    result = correctness_chain.invoke({\"question\": question, \"ground_truth\": ground_truth, \"generated_answer\": generated_answer})\n",
    "    return result.score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test create_correctness_chain\n",
    "question = \"What is the capital of France and Spain?\"\n",
    "ground_truth = \"Paris and Barcelona\"\n",
    "generated_answer = \"Paris\"\n",
    "score = evaluate_correctness(question, ground_truth, generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_prompt = PromptTemplate(\n",
    "input_variables=[\"question\",\"context\", \"generated_answer\"],\n",
    "template=\"\"\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Generated Answer: {generated_answer}\n",
    "\n",
    "Evaluate if the generate answer to the question can be deduced from the context.\n",
    "Score from 0 to 1, where 1 is perfectly faithful and 0 is completely unfaithful.\n",
    "any score between 0 and 1 is acceptable and depends on how faithful the generated answer is.\n",
    "\n",
    "example:\n",
    "Question: What are the capitals of France and Spain?\n",
    "Context: Paris is the capital of France and Madrid is the capital of Spain.\n",
    "Generated Answer: Paris\n",
    "in this case the generated answer is faithful to the context so the score should be 1.\n",
    "\n",
    "example:\n",
    "Question: What are the capital cities of France and Spain?\n",
    "Context: London is the capital of France and Barcelona is the capital of Spain.\n",
    "Generated Answer: London and Barcelona.\n",
    "in this case the generated answer is faithful to the context so the score should be 1.\n",
    "\n",
    "example:\n",
    "Question: What are the capital cities of France and Spain?\n",
    "Context: Paris is the capital of France and Madrid is the capital of Spain.\n",
    "Generated Answer: Paris.\n",
    "in this case the generated answer is faithful to the context so the score should be 1.\n",
    "\n",
    "exmaple:\n",
    "Question: What are the capitals of France and Spain?\n",
    "Context: London is the capital of France and Madrid is the Capital of Spain.\n",
    "Generated Answer: Paris and Madrid.\n",
    "in this case the generated answer is based on the pretrained knowledge of the llm and is not faithful to the context so the score should be 0.\n",
    "\n",
    "example:\n",
    "Question: What is the capital of France and Spain?\n",
    "Context: Monkeys like to eat bananas.\n",
    "Generated Answer: Paris and Madrid.\n",
    "in this case the generated answer is not based on the context so the score should be 0.\n",
    "\"\"\"\n",
    ")\n",
    "faithfulness_chain = faithfulness_prompt | llm.with_structured_output(ResultScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(question, context, generated_answer):\n",
    "    \"\"\"Evaluates if the generate answer to the question can be deduced from the context.\n",
    "\n",
    "    Args:\n",
    "        question: The question.\n",
    "        context: The context.\n",
    "        generated_answer: The generated answer.\n",
    "\n",
    "    Returns:\n",
    "        A float between 0 and 1, where 1 is the best possible score.\n",
    "    \"\"\"\n",
    "    result = faithfulness_chain.invoke({\"question\": question, \"context\": context, \"generated_answer\": generated_answer})\n",
    "    return result.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test create_faithfulness_chain\n",
    "question= \"What are the capital cities of France and Spain?\"\n",
    "context = \"France and Spain are two countries in Europe. The capital of France is Paris and the capital of Spain is Madrid.\"\n",
    "generated_answer = \"Paris\"\n",
    "score = evaluate_faithfulness(question, context, generated_answer)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
