{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation and Meta-Evaluation with GroUSE\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will teach how to evaluate your model of Grounded Question Answering, the common task on the last stage of a RAG pipeline.\n",
    "\n",
    "It will present six metrics that can cover the seven failures modes of Grounded Question Answering.\n",
    "\n",
    "If you want to create your custom evaluator, a second section presents how to evaluate your custom judge LLM on GroUSE unit tests.\n",
    "\n",
    "Check the [GroUSE paper](paper_link.link) for more details.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "## Key Components\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "## Grounded Question Answering\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### 1. Answer Relevancy\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "### 2. Completeness\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "### 3. Faithfulness\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "### 4. Usefulness\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "### 5. Positive Acceptance\n",
    "\n",
    "Percentage of samples whose context had information about the answer and were answered by the model.\n",
    "\n",
    "### 6. Negative Rejection\n",
    "\n",
    "Percentage of samples whose context had no information to answer the question and where the model explained that it could not give an answer to the question.\n",
    "\n",
    "## Benefits of the approach\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "<!-- TODO -->\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "<!-- TODO -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO add Mermaid schema -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grouse import EvaluationSample, GroundedQAEvaluator, MetaEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sample = EvaluationSample(\n",
    "    input=\"What is the capital of France?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"The capital of France is Marseille.[1]\",\n",
    "    expected_output=\"The capital of France is Paris.[1]\",\n",
    "    references=[\"Paris is the capital of France.\"]\n",
    ")\n",
    "irrelevant_sample = EvaluationSample(\n",
    "    input=\"What is the capital of France?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"France is a beautiful country.\",\n",
    "    expected_output=\"The capital of France is Paris.[1]\",\n",
    "    references=[\"Paris is the capital of France.[1]\"]\n",
    ")\n",
    "incomplete_sample = EvaluationSample(\n",
    "    input=\"What is the capital of France and who is the president?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"The capital of France is Paris. [1]\",\n",
    "    expected_output=\"The capital of France is Paris and the president is Emmanuel Macron.\",\n",
    "    references=[\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"Emmanuel Macron is the president of France\"\n",
    "    ]\n",
    ")\n",
    "unfaithful_sample = EvaluationSample(\n",
    "    input=\"What is the capital of France?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"The capital of France is Marseille.\",\n",
    "    expected_output=\"The capital of France is Paris [1]\",\n",
    "    references=[\"Paris is the capital of France.\"]\n",
    ")\n",
    "# Take inspiration from GroUSE\n",
    "useful_sample = EvaluationSample(\n",
    "    input=\"\"\n",
    ")\n",
    "rejected_sample = EvaluationSample(\n",
    "    input=\"What is the capital of France?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"No documents answers the question.\",\n",
    "    expected_output=\"No documents answers the question.\",\n",
    "    references=[\n",
    "        \"Emmanuel Macron is the president of France\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluator = GroundedQAEvaluator()\n",
    "evaluator.evaluate([\n",
    "    good_sample,\n",
    "    irrelevant_sample,\n",
    "    incomplete_sample,\n",
    "    unfaithful_sample,\n",
    "    useful_sample,\n",
    "    rejected_sample,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_RELEVANCY_EVALUATION_PROMPT = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_COMPLETENESS_EVALUATION_PROMPT = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_FAITHFULNESS_EVALUATION_PROMPT = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_USEFULNESS_EVALUATION_PROMPT = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit tests can help you assess the limits of your judge LLM on edge cases but don't guarantee that your judge LLM will be perfect. Be cautious when interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```latex\n",
    "@misc{muller2024grousebenchmarkevaluateevaluators,\n",
    "      title={GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering}, \n",
    "      author={Sacha Muller and Ant√≥nio Loison and Bilel Omrani and Gautier Viaud},\n",
    "      year={2024},\n",
    "      eprint={2409.06595},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL},\n",
    "      url={https://arxiv.org/abs/2409.06595}, \n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
