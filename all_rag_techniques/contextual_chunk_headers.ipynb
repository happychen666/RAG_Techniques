{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Chunk Headers (CCH)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Contextual chunk headers (CCH) is a method of creating document-level and section-level context, and prepending those chunk headers to the chunks prior to embedding them. This gives the embeddings a much more accurate and complete representation of the content and meaning of the text. In our testing, this feature leads to a dramatic improvement in retrieval quality. In addition to increasing the rate at which the correct information is retrieved, CCH also substantially reduces the rate at which irrelevant results show up in the search results. This reduces the rate at which the LLM misinterprets a piece of text in downstream chat and generation applications.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "A large percentage of the problems developers face with RAG comes down to this: Individual chunks oftentimes do not contain sufficient context to be properly used by the retrieval system or the LLM. This leads to the inability to answer questions and, more worryingly, hallucinations.\n",
    "\n",
    "Examples of this problem\n",
    "- Chunks oftentimes refer to their subject via implicit references and pronouns. This causes them to not be retrieved when they should be, or to not be properly understood by the LLM.\n",
    "- Naive chunking can lead to text being split “mid-thought” leaving neither chunk with useful context.\n",
    "- Individual chunks oftentimes only make sense in the context of the entire section or document, and can be misleading when read on their own.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "#### Contextual chunk headers\n",
    "The idea here is to add in higher-level context to the chunk by prepending a chunk header. This chunk header could be as simple as just the document title, or it could use a combination of document title, a concise document summary, and the full hierarchy of section and sub-section titles.\n",
    "\n",
    "## Method Details\n",
    "\n",
    "#### Document title and summary generation\n",
    "We use an LLM to generate a title and descriptive one sentence summary of what the document is about. \n",
    "\n",
    "#### Break into semantically similar sections (optional)\n",
    "Semantic sectioning uses an LLM to break a document into sections. It works by annotating the document with line numbers and then prompting an LLM to identify the starting and ending lines for each “semantically cohesive section.” These sections should be anywhere from a few paragraphs to a few pages long. The sections then get broken into smaller chunks if needed. The LLM is also prompted to generate descriptive titles for each section.\n",
    "\n",
    "#### Chunk the document\n",
    "If semantic sectioning is used, each section is split into chunks. If not, then the document is chunked normally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, you'll need to set the API keys as environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from scipy.stats import beta\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "os.environ[\"CO_API_KEY\"] = os.getenv('CO_API_KEY') # Cohere API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/nike_2023_annual_report.txt\"\n",
    "doc_id = os.path.basename(file_path).split(\".\")[0] # grab the file name without the extension so we can use it as the doc_id\n",
    "\n",
    "kb_id = \"nike_10k\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    document_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions for generating the document title and summary\n",
    "\n",
    "DOCUMENT_TITLE_PROMPT = \"\"\"\n",
    "INSTRUCTIONS\n",
    "What is the title of the following document?\n",
    "\n",
    "Your response MUST be the title of the document, and nothing else. DO NOT respond with anything else.\n",
    "\n",
    "{document_title_guidance}\n",
    "\n",
    "{truncation_message}\n",
    "\n",
    "DOCUMENT\n",
    "{document_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "TRUNCATION_MESSAGE = \"\"\"\n",
    "Also note that the document text provided below is just the first ~{num_words} words of the document. That should be plenty for this task. Your response should still pertain to the entire document, not just the text provided below.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def make_llm_call(chat_messages: list[dict]) -> str:\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=chat_messages,\n",
    "        max_tokens=4000,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    llm_output = response.choices[0].message.content.strip()\n",
    "    return llm_output\n",
    "\n",
    "def truncate_content(content: str, max_tokens: int):\n",
    "    TOKEN_ENCODER = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    tokens = TOKEN_ENCODER.encode(content, disallowed_special=())\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return TOKEN_ENCODER.decode(truncated_tokens), min(len(tokens), max_tokens)\n",
    "\n",
    "def get_document_title(document_text: str, document_title_guidance: str = \"\"):\n",
    "    # truncate the content if it's too long\n",
    "    max_content_tokens = 4000 # if this number changes, also update num_words in the truncation message below\n",
    "    document_text, num_tokens = truncate_content(document_text, max_content_tokens)\n",
    "    if num_tokens < max_content_tokens:\n",
    "        truncation_message = \"\"\n",
    "    else:\n",
    "        truncation_message = TRUNCATION_MESSAGE.format(num_words=3000)\n",
    "\n",
    "    # get document title\n",
    "    prompt = DOCUMENT_TITLE_PROMPT.format(document_title_guidance=document_title_guidance, document_text=document_text, truncation_message=truncation_message)\n",
    "    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    document_title = make_llm_call(chat_messages)\n",
    "    return document_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, chunk_size: int = 800):\n",
    "    \"\"\"\n",
    "    Note: it's very important that chunk overlap is set to 0 here, since results are created by concatenating chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=0, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    chunks = [text.page_content for text in texts]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    \"\"\"\n",
    "    Transformation function to map the absolute relevance value to a value that is more uniformly distributed between 0 and 1\n",
    "    - This is critical for the new version of RSE to work properly, because it utilizes the absolute relevance values to calculate the similarity scores\n",
    "    - The relevance values given by the Cohere reranker tend to be very close to 0 or 1. This beta function used here helps to spread out the values more uniformly.\n",
    "    \"\"\"\n",
    "    a, b = 0.4, 0.4  # These can be adjusted to change the distribution shape\n",
    "    return beta.cdf(x, a, b)\n",
    "\n",
    "def rerank_documents(query: str, documents: list) -> list:\n",
    "    \"\"\"\n",
    "    Use Cohere Rerank API to rerank the search results\n",
    "    \"\"\"\n",
    "    model = \"rerank-english-v3.0\"\n",
    "    client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "    decay_rate = 30\n",
    "\n",
    "    reranked_results = client.rerank(model=model, query=query, documents=documents)\n",
    "    results = reranked_results.results\n",
    "    reranked_indices = [result.index for result in results]\n",
    "    reranked_similarity_scores = [result.relevance_score for result in results] # in order of reranked_indices\n",
    "    \n",
    "    # convert back to order of original documents and calculate the chunk values\n",
    "    similarity_scores = [0] * len(documents)\n",
    "    chunk_values = [0] * len(documents)\n",
    "    for i, index in enumerate(reranked_indices):\n",
    "        absolute_relevance_value = transform(reranked_similarity_scores[i])\n",
    "        similarity_scores[index] = absolute_relevance_value\n",
    "        v = np.exp(-i/decay_rate)*absolute_relevance_value # decay the relevance value based on the rank\n",
    "        chunk_values[index] = v\n",
    "\n",
    "    return similarity_scores, chunk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document title\n",
    "document_title = get_document_title(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "\n",
    "chunks = []\n",
    "\n",
    "section_chunks = split_into_chunks(\n",
    "    document_text, chunk_size=800\n",
    ")\n",
    "for chunk in section_chunks:\n",
    "    chunks.append(\n",
    "        {\n",
    "            \"chunk_text\": chunk,\n",
    "            \"document_title\": document_title\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Title: NIKE, INC. ANNUAL REPORT ON FORM 10-K\n",
      "\n",
      "Given the broad and global scope of our operations, we are particularly vulnerable to the physical risks of climate change, such \n",
      "as shifts in weather patterns. Extreme weather conditions in the areas in which our retail stores, suppliers, manufacturers, \n",
      "customers, distribution centers, offices, headquarters and vendors are located could adversely affect our operating results and \n",
      "financial condition. Moreover, natural disasters such as earthquakes, hurricanes, wildfires, tsunamis, floods or droughts, whether \n",
      "occurring in the United States or abroad, and their related consequences and effects, including energy shortages and public \n",
      "health issues, have in the past temporarily disrupted, and could in the future disrupt, our operations, the operations of our\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "documents_no_context = [] # baseline for comparison\n",
    "for i in range(len(chunks)):\n",
    "    chunk_text = chunks[i][\"chunk_text\"]\n",
    "    document = f\"Document Title: {document_title}\\n\\n{chunk_text}\"\n",
    "    documents.append(document)\n",
    "    documents_no_context.append(chunk_text)\n",
    "\n",
    "chunk_index_to_inspect = 86\n",
    "print (documents[chunk_index_to_inspect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with contextual chunk header: 0.783890004625817\n",
      "Similarity without contextual chunk header: 0.24543648666467205\n"
     ]
    }
   ],
   "source": [
    "query = \"Nike climate change impact\"\n",
    "\n",
    "similarity_scores, chunk_values = rerank_documents(query, [documents[chunk_index_to_inspect], documents_no_context[chunk_index_to_inspect]])\n",
    "\n",
    "print (f\"Similarity with contextual chunk header: {similarity_scores[0]}\")\n",
    "print (f\"Similarity without contextual chunk header: {similarity_scores[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval results\n",
    "\n",
    "#### KITE\n",
    "\n",
    "We evaluated CCH on an end-to-end RAG benchmark we created, called KITE (Knowledge-Intensive Task Evaluation).\n",
    "\n",
    "KITE currently consists of 4 datasets and a total of 50 questions.\n",
    "- **AI Papers** - ~100 academic papers about AI and RAG, downloaded from arXiv in PDF form.\n",
    "- **BVP Cloud 10-Ks** - 10-Ks for all companies in the Bessemer Cloud Index (~70 of them), in PDF form.\n",
    "- **Sourcegraph Company Handbook** - ~800 markdown files, with their original directory structure, downloaded from Sourcegraph's publicly accessible company handbook GitHub [page](https://github.com/sourcegraph/handbook/tree/main/content).\n",
    "- **Supreme Court Opinions** - All Supreme Court opinions from Term Year 2022 (delivered from January '23 to June '23), downloaded from the official Supreme Court [website](https://www.supremecourt.gov/opinions/slipopinion/22) in PDF form.\n",
    "\n",
    "Ground truth answers are included with each sample. Most samples also include grading rubrics. Grading is done on a scale of 0-10 for each question, with a strong LLM doing the grading.\n",
    "\n",
    "We compare CCH with standard Top-k retrieval (k=20). All other parameters remain the same between the two configurations. We use the Cohere 3 reranker, and we use GPT-4o for response generation. The average length of the relevant knowledge string is roughly the same between the two configurations, so cost and latency are similar.\n",
    "\n",
    "|                         | Top-k    | CCH+Top-k    |\n",
    "|-------------------------|----------|--------------|\n",
    "| AI Papers               | 4.5      | 4.7          |\n",
    "| BVP Cloud               | 2.6      | 6.3          |\n",
    "| Sourcegraph             | 5.7      | 5.8          |\n",
    "| Supreme Court Opinions  | 6.1      | 7.4          |\n",
    "| **Average**             | 4.72     | 6.04         |\n",
    "\n",
    "We can see that CCH leads to an improvement in performance on each of the four datasets. The overall average score increases from 4.72 -> 6.04, a 27.9% increase.\n",
    "\n",
    "#### FinanceBench\n",
    "\n",
    "We've also evaluated CCH on FinanceBench, where it contributed to a score of 83%, compared to a baseline score of 19%. For that benchmark, we tested CCH and relevant segment extraction (RSE) jointly, so we can't say exactly how much CCH contributed to that result. But the combination of CCH and RSE clearly leads to substantial accuracy improvements on FinanceBench."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
