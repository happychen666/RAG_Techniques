下面是你给出的 notebook（`document_augmentation.ipynb`）的主要运行流程解释：

---

## 1. **环境准备**
- 安装依赖库：如 `faiss-cpu`, `langchain`, `langchain-openai`, `python-dotenv`。
- 克隆辅助函数和评估模块的仓库，并添加到 Python 路径。

---

## 2. **参数与枚举设置**
- 定义了 `QuestionGeneration` 枚举（`DOCUMENT_LEVEL`、`FRAGMENT_LEVEL`），用于控制问题生成的粒度。
- 各种分割、重叠、生成问题数量参数被写死为常量，方便后续灵活切换。

---

## 3. **主要功能函数定义**
- **`generate_questions(text)`**  
  通过 OpenAI 的大模型，基于输入文本自动生成一组相关问题（数量由参数控制），用于增广文档。
- **`generate_answer(content, question)`**  
  用大模型根据内容和问题生成简洁答案。
- **`split_document(document, chunk_size, chunk_overlap)`**  
  把长文本按设定的 token 数和重叠数分割成多个小块，为后续检索做准备。
- **`clean_and_filter_questions(questions)`**  
  清洗生成的问题，去掉多余编号，只保留以问号结尾的内容。
- **`print_document(comment, document)`**  
  打印文档内容和元信息，方便调试和可视化。

---

## 4. **主处理流程：`process_documents(content, embedding_model)`**
- **分割文档**  
  先将整个内容按 `DOCUMENT_MAX_TOKENS` 分割为若干大块（文档级）。
  每个文档再细分为若干小块（片段级）。
- **增广问题生成**  
  根据 `QUESTION_GENERATION` 参数，决定是在文档级还是片段级生成问题。
  - 如果是片段级，每个片段都生成一组问题，每个问题也作为一个新的增广文档，加入数据集。
  - 如果是文档级，每个文档整体生成一组问题，每个问题也作为新的增广文档。
- **数据集构造**  
  所有原始片段和增广问题都被包装为 `Document` 对象，加入最终的数据集。
- **向量化与检索器创建**  
  用 OpenAI Embedding 模型将所有 `Document` 转换为向量，利用 FAISS 构建向量数据库，并创建检索器返回相关文档。

---

## 5. **使用流程示例**
- 加载 PDF 内容为字符串。
- 用主流程函数处理并构建检索器。
- 用检索器输入查询（如“气候变化是什么？”），返回最相关的增广文档（通常是生成的问题，而不是原始片段）。
- 用问题的父文本片段作为上下文，再调用 `generate_answer` 生成答案。

---

## 6. **总结**
整体流程如下：
1. 读取文档/内容；
2. 分割成片段；
3. 按需生成增广问题；
4. 构建向量数据库用于检索；
5. 用户输入问题后，检索相关增广问题或片段；
6. 用片段上下文生成最终答案。

---

如果你需要某一部分代码的详细解释，或想知道某一函数的具体运行步骤，可以进一步说明！